{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-04T05:43:00.672332Z",
     "iopub.status.busy": "2021-11-04T05:43:00.672044Z",
     "iopub.status.idle": "2021-11-04T05:43:01.999625Z",
     "shell.execute_reply": "2021-11-04T05:43:01.998939Z",
     "shell.execute_reply.started": "2021-11-04T05:43:00.67228Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Logger detectron2 (DEBUG)>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import detectron2\n",
    "from pathlib import Path\n",
    "import random, cv2, os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pycocotools.mask as mask_util\n",
    "# import some common detectron2 utilities\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.engine import DefaultPredictor, DefaultTrainer\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.utils.visualizer import Visualizer, ColorMode\n",
    "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
    "from detectron2.data.datasets import register_coco_instances\n",
    "from detectron2.utils.logger import setup_logger\n",
    "from detectron2.evaluation.evaluator import DatasetEvaluator\n",
    "from detectron2.structures import polygons_to_bitmask\n",
    "from detectron2.evaluation import inference_on_dataset, print_csv_format\n",
    "from detectron2.utils import comm\n",
    "from detectron2.modeling.meta_arch.rcnn import GeneralizedRCNN\n",
    "from detectron2.engine import BestCheckpointer\n",
    "from detectron2.checkpoint import DetectionCheckpointer\n",
    "from glob import glob\n",
    "from src.swin.swint.config import add_swinl_384_config\n",
    "from src.evaluator import MAPIOUEvaluator\n",
    "import torch\n",
    "\n",
    "setup_logger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[12/22 11:07:01 d2.data.datasets.coco]: \u001b[0mLoading LIVECell_dataset_2021/annotations/LIVECell/livecell_coco_train.json takes 10.29 seconds.\n",
      "\u001b[32m[12/22 11:07:01 d2.data.datasets.coco]: \u001b[0mLoaded 3253 images in COCO format from LIVECell_dataset_2021/annotations/LIVECell/livecell_coco_train.json\n"
     ]
    }
   ],
   "source": [
    "dataDir=Path('LIVECell_dataset_2021/images')\n",
    "cfg = get_cfg()\n",
    "register_coco_instances('sartorius_train',{}, 'LIVECell_dataset_2021/annotations/LIVECell/livecell_coco_train.json', dataDir/'livecell_train_val_images')\n",
    "register_coco_instances('sartorius_val',{},'LIVECell_dataset_2021/annotations/LIVECell/livecell_coco_val.json', dataDir/'livecell_train_val_images')\n",
    "register_coco_instances('sartorius_test',{}, 'LIVECell_dataset_2021/annotations/LIVECell/livecell_coco_test.json', dataDir/'livecell_test_images')\n",
    "metadata = MetadataCatalog.get('sartorius_train')\n",
    "train_ds = DatasetCatalog.get('sartorius_train')\n",
    "config_name = \"lib/swin/configs/SwinT/mask_rcnn_swint_T_FPN_3x.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-04T05:43:06.875503Z",
     "iopub.status.busy": "2021-11-04T05:43:06.875274Z",
     "iopub.status.idle": "2021-11-04T05:43:06.893155Z",
     "shell.execute_reply": "2021-11-04T05:43:06.892489Z",
     "shell.execute_reply.started": "2021-11-04T05:43:06.875472Z"
    }
   },
   "outputs": [],
   "source": [
    "class Trainer(DefaultTrainer):\n",
    "    @classmethod\n",
    "    def build_evaluator(cls, cfg, dataset_name, output_folder=None):\n",
    "        return MAPIOUEvaluator(dataset_name)\n",
    "    \n",
    "    def build_hooks(self):\n",
    "        ret = super().build_hooks()\n",
    "        ret.append(BestCheckpointer(cfg.TEST.EVAL_PERIOD,\n",
    "                                    self.checkpointer,\n",
    "                                    'segm/AP', \n",
    "                                    file_prefix=f'{os.path.basename(config_name).rstrip(\".yaml\")}_best'))\n",
    "        return ret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = get_cfg()\n",
    "add_swinl_384_config(cfg)\n",
    "cfg.merge_from_file(config_name)\n",
    "\n",
    "cfg.DATASETS.TRAIN = (\"sartorius_train\", \"sartorius_test\")\n",
    "cfg.DATASETS.TEST = (\"sartorius_val\",)\n",
    "\n",
    "\n",
    "cfg.MODEL.WEIGHTS = \"lib/swin/swin_large_patch4_window12_384_22kto1k_d2.pth\"\n",
    "\n",
    "cfg.SOLVER.IMS_PER_BATCH = 2\n",
    "cfg.DATALOADER.NUM_WORKERS = 10\n",
    "    \n",
    "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 256   \n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1\n",
    "cfg.MODEL.RETINANET.NUM_CLASSES = 1\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = .5\n",
    "    \n",
    "cfg.MODEL.RPN.BBOX_REG_LOSS_TYPE = \"ciou\"\n",
    "cfg.MODEL.ROI_BOX_HEAD.BBOX_REG_LOSS_TYPE = \"ciou\"\n",
    "\n",
    "cfg.INPUT.CROP.ENABLED = True\n",
    "cfg.INPUT.CROP.SIZE = [0.85, 0.97]\n",
    "\n",
    "cfg.SOLVER.BASE_LR = 0.02\n",
    "     \n",
    "cfg.SOLVER.CHECKPOINT_PERIOD = 1000  \n",
    "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128   \n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = .5\n",
    "cfg.TEST.EVAL_PERIOD = 1000  # Once per epoch\n",
    "cfg.MODEL.RPN.BBOX_REG_LOSS_TYPE = \"ciou\"\n",
    "cfg.MODEL.ROI_BOX_HEAD.BBOX_REG_LOSS_TYPE = \"ciou\"\n",
    "cfg.SOLVER.AMP.ENABLED = True\n",
    "\n",
    "cfg.OUTPUT_DIR = f'swin_l_384_weights'\n",
    "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[12/22 11:08:45 d2.engine.defaults]: \u001b[0mModel:\n",
      "GeneralizedRCNN(\n",
      "  (backbone): FPN(\n",
      "    (fpn_lateral2): Conv2d(192, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral3): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral4): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral5): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (top_block): LastLevelMaxPool()\n",
      "    (bottom_up): SwinTransformer(\n",
      "      (patch_embed): PatchEmbed(\n",
      "        (proj): Conv2d(3, 192, kernel_size=(4, 4), stride=(4, 4))\n",
      "        (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "      (layers): ModuleList(\n",
      "        (0): BasicLayer(\n",
      "          (blocks): ModuleList(\n",
      "            (0): SwinTransformerBlock(\n",
      "              (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "              (attn): WindowAttention(\n",
      "                (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                (softmax): Softmax(dim=-1)\n",
      "              )\n",
      "              (drop_path): Identity()\n",
      "              (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): Mlp(\n",
      "                (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "                (act): GELU()\n",
      "                (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "                (drop): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (1): SwinTransformerBlock(\n",
      "              (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "              (attn): WindowAttention(\n",
      "                (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
      "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                (proj): Linear(in_features=192, out_features=192, bias=True)\n",
      "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                (softmax): Softmax(dim=-1)\n",
      "              )\n",
      "              (drop_path): DropPath()\n",
      "              (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): Mlp(\n",
      "                (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
      "                (act): GELU()\n",
      "                (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
      "                (drop): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (downsample): PatchMerging(\n",
      "            (reduction): Linear(in_features=768, out_features=384, bias=False)\n",
      "            (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (1): BasicLayer(\n",
      "          (blocks): ModuleList(\n",
      "            (0): SwinTransformerBlock(\n",
      "              (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "              (attn): WindowAttention(\n",
      "                (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                (softmax): Softmax(dim=-1)\n",
      "              )\n",
      "              (drop_path): DropPath()\n",
      "              (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): Mlp(\n",
      "                (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "                (act): GELU()\n",
      "                (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "                (drop): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (1): SwinTransformerBlock(\n",
      "              (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "              (attn): WindowAttention(\n",
      "                (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
      "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                (proj): Linear(in_features=384, out_features=384, bias=True)\n",
      "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                (softmax): Softmax(dim=-1)\n",
      "              )\n",
      "              (drop_path): DropPath()\n",
      "              (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): Mlp(\n",
      "                (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
      "                (act): GELU()\n",
      "                (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
      "                (drop): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (downsample): PatchMerging(\n",
      "            (reduction): Linear(in_features=1536, out_features=768, bias=False)\n",
      "            (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (2): BasicLayer(\n",
      "          (blocks): ModuleList(\n",
      "            (0): SwinTransformerBlock(\n",
      "              (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (attn): WindowAttention(\n",
      "                (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                (softmax): Softmax(dim=-1)\n",
      "              )\n",
      "              (drop_path): DropPath()\n",
      "              (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): Mlp(\n",
      "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (act): GELU()\n",
      "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (drop): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (1): SwinTransformerBlock(\n",
      "              (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (attn): WindowAttention(\n",
      "                (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                (softmax): Softmax(dim=-1)\n",
      "              )\n",
      "              (drop_path): DropPath()\n",
      "              (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): Mlp(\n",
      "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (act): GELU()\n",
      "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (drop): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (2): SwinTransformerBlock(\n",
      "              (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (attn): WindowAttention(\n",
      "                (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                (softmax): Softmax(dim=-1)\n",
      "              )\n",
      "              (drop_path): DropPath()\n",
      "              (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): Mlp(\n",
      "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (act): GELU()\n",
      "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (drop): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (3): SwinTransformerBlock(\n",
      "              (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (attn): WindowAttention(\n",
      "                (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                (softmax): Softmax(dim=-1)\n",
      "              )\n",
      "              (drop_path): DropPath()\n",
      "              (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): Mlp(\n",
      "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (act): GELU()\n",
      "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (drop): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (4): SwinTransformerBlock(\n",
      "              (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (attn): WindowAttention(\n",
      "                (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                (softmax): Softmax(dim=-1)\n",
      "              )\n",
      "              (drop_path): DropPath()\n",
      "              (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): Mlp(\n",
      "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (act): GELU()\n",
      "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (drop): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (5): SwinTransformerBlock(\n",
      "              (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (attn): WindowAttention(\n",
      "                (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                (softmax): Softmax(dim=-1)\n",
      "              )\n",
      "              (drop_path): DropPath()\n",
      "              (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): Mlp(\n",
      "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (act): GELU()\n",
      "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (drop): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (6): SwinTransformerBlock(\n",
      "              (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (attn): WindowAttention(\n",
      "                (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                (softmax): Softmax(dim=-1)\n",
      "              )\n",
      "              (drop_path): DropPath()\n",
      "              (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): Mlp(\n",
      "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (act): GELU()\n",
      "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (drop): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (7): SwinTransformerBlock(\n",
      "              (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (attn): WindowAttention(\n",
      "                (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                (softmax): Softmax(dim=-1)\n",
      "              )\n",
      "              (drop_path): DropPath()\n",
      "              (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): Mlp(\n",
      "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (act): GELU()\n",
      "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (drop): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (8): SwinTransformerBlock(\n",
      "              (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (attn): WindowAttention(\n",
      "                (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                (softmax): Softmax(dim=-1)\n",
      "              )\n",
      "              (drop_path): DropPath()\n",
      "              (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): Mlp(\n",
      "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (act): GELU()\n",
      "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (drop): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (9): SwinTransformerBlock(\n",
      "              (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (attn): WindowAttention(\n",
      "                (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                (softmax): Softmax(dim=-1)\n",
      "              )\n",
      "              (drop_path): DropPath()\n",
      "              (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): Mlp(\n",
      "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (act): GELU()\n",
      "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (drop): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (10): SwinTransformerBlock(\n",
      "              (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (attn): WindowAttention(\n",
      "                (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                (softmax): Softmax(dim=-1)\n",
      "              )\n",
      "              (drop_path): DropPath()\n",
      "              (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): Mlp(\n",
      "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (act): GELU()\n",
      "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (drop): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (11): SwinTransformerBlock(\n",
      "              (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (attn): WindowAttention(\n",
      "                (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                (softmax): Softmax(dim=-1)\n",
      "              )\n",
      "              (drop_path): DropPath()\n",
      "              (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): Mlp(\n",
      "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (act): GELU()\n",
      "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (drop): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (12): SwinTransformerBlock(\n",
      "              (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (attn): WindowAttention(\n",
      "                (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                (softmax): Softmax(dim=-1)\n",
      "              )\n",
      "              (drop_path): DropPath()\n",
      "              (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): Mlp(\n",
      "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (act): GELU()\n",
      "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (drop): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (13): SwinTransformerBlock(\n",
      "              (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (attn): WindowAttention(\n",
      "                (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                (softmax): Softmax(dim=-1)\n",
      "              )\n",
      "              (drop_path): DropPath()\n",
      "              (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): Mlp(\n",
      "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (act): GELU()\n",
      "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (drop): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (14): SwinTransformerBlock(\n",
      "              (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (attn): WindowAttention(\n",
      "                (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                (softmax): Softmax(dim=-1)\n",
      "              )\n",
      "              (drop_path): DropPath()\n",
      "              (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): Mlp(\n",
      "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (act): GELU()\n",
      "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (drop): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (15): SwinTransformerBlock(\n",
      "              (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (attn): WindowAttention(\n",
      "                (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                (softmax): Softmax(dim=-1)\n",
      "              )\n",
      "              (drop_path): DropPath()\n",
      "              (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): Mlp(\n",
      "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (act): GELU()\n",
      "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (drop): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (16): SwinTransformerBlock(\n",
      "              (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (attn): WindowAttention(\n",
      "                (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                (softmax): Softmax(dim=-1)\n",
      "              )\n",
      "              (drop_path): DropPath()\n",
      "              (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): Mlp(\n",
      "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (act): GELU()\n",
      "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (drop): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (17): SwinTransformerBlock(\n",
      "              (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (attn): WindowAttention(\n",
      "                (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                (softmax): Softmax(dim=-1)\n",
      "              )\n",
      "              (drop_path): DropPath()\n",
      "              (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): Mlp(\n",
      "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (act): GELU()\n",
      "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (drop): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (downsample): PatchMerging(\n",
      "            (reduction): Linear(in_features=3072, out_features=1536, bias=False)\n",
      "            (norm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "        (3): BasicLayer(\n",
      "          (blocks): ModuleList(\n",
      "            (0): SwinTransformerBlock(\n",
      "              (norm1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "              (attn): WindowAttention(\n",
      "                (qkv): Linear(in_features=1536, out_features=4608, bias=True)\n",
      "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                (proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                (softmax): Softmax(dim=-1)\n",
      "              )\n",
      "              (drop_path): DropPath()\n",
      "              (norm2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): Mlp(\n",
      "                (fc1): Linear(in_features=1536, out_features=6144, bias=True)\n",
      "                (act): GELU()\n",
      "                (fc2): Linear(in_features=6144, out_features=1536, bias=True)\n",
      "                (drop): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "            (1): SwinTransformerBlock(\n",
      "              (norm1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "              (attn): WindowAttention(\n",
      "                (qkv): Linear(in_features=1536, out_features=4608, bias=True)\n",
      "                (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "                (proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
      "                (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "                (softmax): Softmax(dim=-1)\n",
      "              )\n",
      "              (drop_path): DropPath()\n",
      "              (norm2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "              (mlp): Mlp(\n",
      "                (fc1): Linear(in_features=1536, out_features=6144, bias=True)\n",
      "                (act): GELU()\n",
      "                (fc2): Linear(in_features=6144, out_features=1536, bias=True)\n",
      "                (drop): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm0): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm3): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (proposal_generator): RPN(\n",
      "    (rpn_head): StandardRPNHead(\n",
      "      (conv): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (anchor_generator): DefaultAnchorGenerator(\n",
      "      (cell_anchors): BufferList()\n",
      "    )\n",
      "  )\n",
      "  (roi_heads): CascadeROIHeads(\n",
      "    (box_pooler): ROIPooler(\n",
      "      (level_poolers): ModuleList(\n",
      "        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
      "        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
      "        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
      "        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
      "      )\n",
      "    )\n",
      "    (box_head): ModuleList(\n",
      "      (0): FastRCNNConvFCHead(\n",
      "        (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "        (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n",
      "        (fc_relu1): ReLU()\n",
      "        (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (fc_relu2): ReLU()\n",
      "      )\n",
      "      (1): FastRCNNConvFCHead(\n",
      "        (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "        (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n",
      "        (fc_relu1): ReLU()\n",
      "        (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (fc_relu2): ReLU()\n",
      "      )\n",
      "      (2): FastRCNNConvFCHead(\n",
      "        (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "        (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n",
      "        (fc_relu1): ReLU()\n",
      "        (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (fc_relu2): ReLU()\n",
      "      )\n",
      "    )\n",
      "    (box_predictor): ModuleList(\n",
      "      (0): FastRCNNOutputLayers(\n",
      "        (cls_score): Linear(in_features=1024, out_features=2, bias=True)\n",
      "        (bbox_pred): Linear(in_features=1024, out_features=4, bias=True)\n",
      "      )\n",
      "      (1): FastRCNNOutputLayers(\n",
      "        (cls_score): Linear(in_features=1024, out_features=2, bias=True)\n",
      "        (bbox_pred): Linear(in_features=1024, out_features=4, bias=True)\n",
      "      )\n",
      "      (2): FastRCNNOutputLayers(\n",
      "        (cls_score): Linear(in_features=1024, out_features=2, bias=True)\n",
      "        (bbox_pred): Linear(in_features=1024, out_features=4, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (mask_pooler): ROIPooler(\n",
      "      (level_poolers): ModuleList(\n",
      "        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
      "        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
      "        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
      "        (3): ROIAlign(output_size=(14, 14), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
      "      )\n",
      "    )\n",
      "    (mask_head): MaskRCNNConvUpsampleHead(\n",
      "      (mask_fcn1): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (mask_fcn2): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (mask_fcn3): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (mask_fcn4): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (mask_fcn5): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (mask_fcn6): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (mask_fcn7): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (mask_fcn8): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "        (norm): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (deconv): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n",
      "      (deconv_relu): ReLU()\n",
      "      (predictor): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\u001b[32m[12/22 11:08:58 d2.data.datasets.coco]: \u001b[0mLoading LIVECell_dataset_2021/annotations/LIVECell/livecell_coco_train.json takes 12.45 seconds.\n",
      "\u001b[32m[12/22 11:08:58 d2.data.datasets.coco]: \u001b[0mLoaded 3253 images in COCO format from LIVECell_dataset_2021/annotations/LIVECell/livecell_coco_train.json\n",
      "\u001b[32m[12/22 11:09:06 d2.data.datasets.coco]: \u001b[0mLoading LIVECell_dataset_2021/annotations/LIVECell/livecell_coco_test.json takes 6.67 seconds.\n",
      "\u001b[32m[12/22 11:09:06 d2.data.datasets.coco]: \u001b[0mLoaded 1564 images in COCO format from LIVECell_dataset_2021/annotations/LIVECell/livecell_coco_test.json\n",
      "\u001b[32m[12/22 11:09:07 d2.data.build]: \u001b[0mRemoved 0 images with no usable annotations. 4817 images left.\n",
      "\u001b[32m[12/22 11:09:07 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in training: [RandomCrop(crop_type='relative_range', crop_size=[0.85, 0.97]), ResizeShortestEdge(short_edge_length=(640, 864), max_size=1440, sample_style='range'), RandomFlip()]\n",
      "\u001b[32m[12/22 11:09:07 d2.data.build]: \u001b[0mUsing training sampler TrainingSampler\n",
      "\u001b[32m[12/22 11:09:07 d2.data.common]: \u001b[0mSerializing 4817 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[12/22 11:09:09 d2.data.common]: \u001b[0mSerialized dataset takes 707.48 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some model parameters or buffers are not found in the checkpoint:\n",
      "\u001b[34mbackbone.bottom_up.norm0.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.norm1.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.norm2.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.bottom_up.norm3.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.fpn_lateral2.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.fpn_lateral3.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.fpn_lateral4.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.fpn_lateral5.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.fpn_output2.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.fpn_output3.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.fpn_output4.{bias, weight}\u001b[0m\n",
      "\u001b[34mbackbone.fpn_output5.{bias, weight}\u001b[0m\n",
      "\u001b[34mproposal_generator.rpn_head.anchor_deltas.{bias, weight}\u001b[0m\n",
      "\u001b[34mproposal_generator.rpn_head.conv.{bias, weight}\u001b[0m\n",
      "\u001b[34mproposal_generator.rpn_head.objectness_logits.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.box_head.0.fc1.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.box_head.0.fc2.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.box_head.1.fc1.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.box_head.1.fc2.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.box_head.2.fc1.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.box_head.2.fc2.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.box_predictor.0.bbox_pred.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.box_predictor.0.cls_score.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.box_predictor.1.bbox_pred.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.box_predictor.1.cls_score.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.box_predictor.2.bbox_pred.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.box_predictor.2.cls_score.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.mask_head.deconv.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.mask_head.mask_fcn1.norm.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.mask_head.mask_fcn1.weight\u001b[0m\n",
      "\u001b[34mroi_heads.mask_head.mask_fcn2.norm.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.mask_head.mask_fcn2.weight\u001b[0m\n",
      "\u001b[34mroi_heads.mask_head.mask_fcn3.norm.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.mask_head.mask_fcn3.weight\u001b[0m\n",
      "\u001b[34mroi_heads.mask_head.mask_fcn4.norm.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.mask_head.mask_fcn4.weight\u001b[0m\n",
      "\u001b[34mroi_heads.mask_head.mask_fcn5.norm.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.mask_head.mask_fcn5.weight\u001b[0m\n",
      "\u001b[34mroi_heads.mask_head.mask_fcn6.norm.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.mask_head.mask_fcn6.weight\u001b[0m\n",
      "\u001b[34mroi_heads.mask_head.mask_fcn7.norm.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.mask_head.mask_fcn7.weight\u001b[0m\n",
      "\u001b[34mroi_heads.mask_head.mask_fcn8.norm.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.mask_head.mask_fcn8.weight\u001b[0m\n",
      "\u001b[34mroi_heads.mask_head.predictor.{bias, weight}\u001b[0m\n",
      "The checkpoint state_dict contains keys that are not used by the model:\n",
      "  \u001b[35mbackbone.bottom_up.norm.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.head.{bias, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.layers.0.blocks.1.attn_mask\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.layers.1.blocks.1.attn_mask\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.layers.2.blocks.1.attn_mask\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.layers.2.blocks.3.attn_mask\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.layers.2.blocks.5.attn_mask\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.layers.2.blocks.7.attn_mask\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.layers.2.blocks.9.attn_mask\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.layers.2.blocks.11.attn_mask\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.layers.2.blocks.13.attn_mask\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.layers.2.blocks.15.attn_mask\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.layers.2.blocks.17.attn_mask\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[12/22 11:09:11 d2.engine.train_loop]: \u001b[0mStarting training from iteration 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nmark/kaggle/sartorius/detectron_sartorius/venv/lib/python3.8/site-packages/fvcore/transforms/transform.py:724: ShapelyDeprecationWarning: Iteration over multi-part geometries is deprecated and will be removed in Shapely 2.0. Use the `geoms` property to access the constituent parts of a multi-part geometry.\n",
      "  for poly in cropped:\n",
      "/home/nmark/kaggle/sartorius/detectron_sartorius/venv/lib/python3.8/site-packages/fvcore/transforms/transform.py:724: ShapelyDeprecationWarning: Iteration over multi-part geometries is deprecated and will be removed in Shapely 2.0. Use the `geoms` property to access the constituent parts of a multi-part geometry.\n",
      "  for poly in cropped:\n",
      "/home/nmark/kaggle/sartorius/detectron_sartorius/venv/lib/python3.8/site-packages/fvcore/transforms/transform.py:724: ShapelyDeprecationWarning: Iteration over multi-part geometries is deprecated and will be removed in Shapely 2.0. Use the `geoms` property to access the constituent parts of a multi-part geometry.\n",
      "  for poly in cropped:\n",
      "/home/nmark/kaggle/sartorius/detectron_sartorius/venv/lib/python3.8/site-packages/fvcore/transforms/transform.py:724: ShapelyDeprecationWarning: Iteration over multi-part geometries is deprecated and will be removed in Shapely 2.0. Use the `geoms` property to access the constituent parts of a multi-part geometry.\n",
      "  for poly in cropped:\n",
      "/home/nmark/kaggle/sartorius/detectron_sartorius/venv/lib/python3.8/site-packages/fvcore/transforms/transform.py:724: ShapelyDeprecationWarning: Iteration over multi-part geometries is deprecated and will be removed in Shapely 2.0. Use the `geoms` property to access the constituent parts of a multi-part geometry.\n",
      "  for poly in cropped:\n",
      "/home/nmark/kaggle/sartorius/detectron_sartorius/venv/lib/python3.8/site-packages/detectron2/structures/image_list.py:88: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  max_size = (max_size + (stride - 1)) // stride * stride\n",
      "/home/nmark/kaggle/sartorius/detectron_sartorius/venv/lib/python3.8/site-packages/fvcore/transforms/transform.py:724: ShapelyDeprecationWarning: Iteration over multi-part geometries is deprecated and will be removed in Shapely 2.0. Use the `geoms` property to access the constituent parts of a multi-part geometry.\n",
      "  for poly in cropped:\n",
      "/home/nmark/kaggle/sartorius/detectron_sartorius/venv/lib/python3.8/site-packages/fvcore/transforms/transform.py:724: ShapelyDeprecationWarning: Iteration over multi-part geometries is deprecated and will be removed in Shapely 2.0. Use the `geoms` property to access the constituent parts of a multi-part geometry.\n",
      "  for poly in cropped:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[12/22 11:09:12 d2.utils.memory]: \u001b[0mAttempting to copy inputs of <function pairwise_iou at 0x7f2adf0bc940> to CPU due to CUDA OOM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nmark/kaggle/sartorius/detectron_sartorius/venv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:129: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[4m\u001b[5m\u001b[31mERROR\u001b[0m \u001b[32m[12/22 11:09:14 d2.engine.train_loop]: \u001b[0mException during training:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nmark/kaggle/sartorius/detectron_sartorius/venv/lib/python3.8/site-packages/detectron2/engine/train_loop.py\", line 149, in train\n",
      "    self.run_step()\n",
      "  File \"/home/nmark/kaggle/sartorius/detectron_sartorius/venv/lib/python3.8/site-packages/detectron2/engine/defaults.py\", line 494, in run_step\n",
      "    self._trainer.run_step()\n",
      "  File \"/home/nmark/kaggle/sartorius/detectron_sartorius/venv/lib/python3.8/site-packages/detectron2/engine/train_loop.py\", line 395, in run_step\n",
      "    loss_dict = self.model(data)\n",
      "  File \"/home/nmark/kaggle/sartorius/detectron_sartorius/venv/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/nmark/kaggle/sartorius/detectron_sartorius/venv/lib/python3.8/site-packages/detectron2/modeling/meta_arch/rcnn.py\", line 154, in forward\n",
      "    features = self.backbone(images.tensor)\n",
      "  File \"/home/nmark/kaggle/sartorius/detectron_sartorius/venv/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/nmark/kaggle/sartorius/detectron_sartorius/venv/lib/python3.8/site-packages/detectron2/modeling/backbone/fpn.py\", line 126, in forward\n",
      "    bottom_up_features = self.bottom_up(x)\n",
      "  File \"/home/nmark/kaggle/sartorius/detectron_sartorius/venv/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/nmark/kaggle/sartorius/detectron_sartorius/lib/swin/swint/swin_transformer.py\", line 605, in forward\n",
      "    x_out, H, W, x, Wh, Ww = layer(x, Wh, Ww)\n",
      "  File \"/home/nmark/kaggle/sartorius/detectron_sartorius/venv/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/nmark/kaggle/sartorius/detectron_sartorius/lib/swin/swint/swin_transformer.py\", line 385, in forward\n",
      "    x = blk(x, attn_mask)\n",
      "  File \"/home/nmark/kaggle/sartorius/detectron_sartorius/venv/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/nmark/kaggle/sartorius/detectron_sartorius/lib/swin/swint/swin_transformer.py\", line 227, in forward\n",
      "    attn_windows = self.attn(x_windows, mask=attn_mask)  # nW*B, window_size*window_size, C\n",
      "  File \"/home/nmark/kaggle/sartorius/detectron_sartorius/venv/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/nmark/kaggle/sartorius/detectron_sartorius/lib/swin/swint/swin_transformer.py\", line 141, in forward\n",
      "    attn = self.softmax(attn)\n",
      "  File \"/home/nmark/kaggle/sartorius/detectron_sartorius/venv/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 1102, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home/nmark/kaggle/sartorius/detectron_sartorius/venv/lib/python3.8/site-packages/torch/nn/modules/activation.py\", line 1226, in forward\n",
      "    return F.softmax(input, self.dim, _stacklevel=5)\n",
      "  File \"/home/nmark/kaggle/sartorius/detectron_sartorius/venv/lib/python3.8/site-packages/torch/nn/functional.py\", line 1680, in softmax\n",
      "    ret = input.softmax(dim)\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 92.00 MiB (GPU 0; 23.70 GiB total capacity; 20.47 GiB already allocated; 94.56 MiB free; 20.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
      "\u001b[32m[12/22 11:09:14 d2.engine.hooks]: \u001b[0mTotal training time: 0:00:00 (0:00:00 on hooks)\n",
      "\u001b[32m[12/22 11:09:14 d2.utils.events]: \u001b[0m iter: 1  total_loss: 3.966  loss_cls_stage0: 0.7492  loss_box_reg_stage0: 0.03741  loss_cls_stage1: 0.7853  loss_box_reg_stage1: 0.01247  loss_cls_stage2: 0.725  loss_box_reg_stage2: 0.005755  loss_mask: 0.6936  loss_rpn_cls: 0.6968  loss_rpn_loc: 0.2609  data_time: 0.6567  lr: 2e-05  max_mem: 21004M\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 92.00 MiB (GPU 0; 23.70 GiB total capacity; 20.47 GiB already allocated; 94.56 MiB free; 20.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8014/2358231036.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# without data augmentation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresume_or_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresume\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/kaggle/sartorius/detectron_sartorius/venv/lib/python3.8/site-packages/detectron2/engine/defaults.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    482\u001b[0m             \u001b[0mOrderedDict\u001b[0m \u001b[0mof\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mevaluation\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0menabled\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mOtherwise\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m         \"\"\"\n\u001b[0;32m--> 484\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    485\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTEST\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEXPECTED_RESULTS\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcomm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_main_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             assert hasattr(\n",
      "\u001b[0;32m~/kaggle/sartorius/detectron_sartorius/venv/lib/python3.8/site-packages/detectron2/engine/train_loop.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, start_iter, max_iter)\u001b[0m\n\u001b[1;32m    147\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbefore_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mafter_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m                 \u001b[0;31m# self.iter == max_iter can be used by `after_train` to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/kaggle/sartorius/detectron_sartorius/venv/lib/python3.8/site-packages/detectron2/engine/defaults.py\u001b[0m in \u001b[0;36mrun_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    492\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 494\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    495\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/kaggle/sartorius/detectron_sartorius/venv/lib/python3.8/site-packages/detectron2/engine/train_loop.py\u001b[0m in \u001b[0;36mrun_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mautocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m             \u001b[0mloss_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    396\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m                 \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/kaggle/sartorius/detectron_sartorius/venv/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/kaggle/sartorius/detectron_sartorius/venv/lib/python3.8/site-packages/detectron2/modeling/meta_arch/rcnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, batched_inputs)\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0mgt_instances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackbone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproposal_generator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/kaggle/sartorius/detectron_sartorius/venv/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/kaggle/sartorius/detectron_sartorius/venv/lib/python3.8/site-packages/detectron2/modeling/backbone/fpn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    124\u001b[0m                 \u001b[0;34m[\u001b[0m\u001b[0;34m\"p2\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"p3\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"p6\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \"\"\"\n\u001b[0;32m--> 126\u001b[0;31m         \u001b[0mbottom_up_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbottom_up\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mprev_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlateral_convs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbottom_up_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/kaggle/sartorius/detectron_sartorius/venv/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/kaggle/sartorius/detectron_sartorius/lib/swin/swint/swin_transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    603\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m             \u001b[0mlayer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m             \u001b[0mx_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mWw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m             \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'stage{i+2}'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_features\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/kaggle/sartorius/detectron_sartorius/venv/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/kaggle/sartorius/detectron_sartorius/lib/swin/swint/swin_transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, H, W)\u001b[0m\n\u001b[1;32m    383\u001b[0m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownsample\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m             \u001b[0mx_down\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownsample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/kaggle/sartorius/detectron_sartorius/venv/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/kaggle/sartorius/detectron_sartorius/lib/swin/swint/swin_transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, mask_matrix)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0;31m# W-MSA/SW-MSA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m         \u001b[0mattn_windows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_windows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattn_mask\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# nW*B, window_size*window_size, C\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m         \u001b[0;31m# merge windows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/kaggle/sartorius/detectron_sartorius/venv/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/kaggle/sartorius/detectron_sartorius/lib/swin/swint/swin_transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0mattn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0mattn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mattn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn_drop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/kaggle/sartorius/detectron_sartorius/venv/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/kaggle/sartorius/detectron_sartorius/venv/lib/python3.8/site-packages/torch/nn/modules/activation.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m   1224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1225\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1226\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1228\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/kaggle/sartorius/detectron_sartorius/venv/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36msoftmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   1678\u001b[0m         \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_softmax_dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"softmax\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacklevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1679\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1680\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1681\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1682\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 92.00 MiB (GPU 0; 23.70 GiB total capacity; 20.47 GiB already allocated; 94.56 MiB free; 20.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(cfg) # without data augmentation\n",
    "trainer.resume_or_load(resume=False)\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
